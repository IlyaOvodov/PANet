# Path Aggregation Network (PANet) for PyTorch 1.2 and AMD Radeon Open Compute (ROCm)

This is an adaptation of
[Path Aggregation Network for Instance Segmentation (PANet)](https://github.com/ShuLiu1993/PANet)
by [Shu Liu](http://shuliu.me), Lu Qi, Haifang Qin, [Jianping Shi](https://shijianping.me/), [Jiaya Jia](http://jiaya.me/)
for PyTorch 1.2 and AMD Radeon Open Compute (ROCm).

This implementation can be installed both on **NVIDIA CUDA** and **AMD ROCm**. 

To install:
```shell
git clone https://github.com/anakham/PANet.git
cd PANet/lib
sh make.sh
```

See the [original README](https://github.com/ShuLiu1993/PANet/blob/master/README.md)
for more details about installation and usage.

Changes required for a PANet PyTorch 1.2 project to start working on ROCm can be found in the 
[commit 3c08e9](https://github.com/anakham/PANet/commit/3c08e9406d36d1ce9b513ad3d7b625cb871b04f9).
You may consider it as a template for adaptation of CUDA projects to ROCm-capable platforms
in case these projects use JIT compilation modules.

Some improvements in the original project have also been done:
- DATA_DIR can be added to config file to allow data being placed in an arbitrary
directory, not only in `<project root>/data` directory
- train parameters are stored in a `config_and_args.cfg` file in a human-readable format
- some fixes have been done to enable operation with [Cityscapes](https://www.cityscapes-dataset.com) dataset
- added `--skip_top_layers` command-line argument to enable loading pretrained model with
different classes number (i.e. COCO pretrained model for training on Cityscapes dataset)
- [config file](https://github.com/anakham/PANet/blob/master/configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml)
for training on the Cityscapes dataset as described in the [original paper](https://arxiv.org/pdf/1803.01534v4.pdf)
(except what 4 GPUs are used instead of 8) is added
- handling of `--iter_size` parameter was changed to make more clear splitting effective batch size to smaller parts
for running with lower number of GPUs or GPUs with lower memory
to get similar result (different only in batch norm) with same config file
and *same* checkpointing period

## Training on synthetic data
- means for working with data generated by **Chameleon AI Tools
[Highwai simulator](https://www.mindtech.global/products)** by [Mindtech](https://www.mindtech.global)
 were added
- Domain-Adversarial Training of Neural Networks ([DANN](https://arxiv.org/abs/1505.07818))
domain adaptation is implemented

**See branch [Chameleon](https://github.com/anakham/PANet/tree/DANN) for details.**

  




