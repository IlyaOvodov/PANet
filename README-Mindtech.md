# Instance segmentation on the Cityscapes dataset using Path Aggregation Network (PANet) and synthetic data generated by Chameleon AI Tools Highwai simulator by Mindtech

We evaluated possible ways of using synthetically generated images to improve quality of solving ML tasks.  
We used [Path Aggregation Network for Instance Segmentation (PANet)](https://github.com/ShuLiu1993/PANet) as a basis 
for solving the instance segmentation task on Cityscapes.
Two approaches were investigated:
- Style transfer
- Domain adversarial training  

## Preface

PANet takes one of the leading positions the [Citiscapes benchmark on Instance-Level Semantic Labeling Task](https://www.cityscapes-dataset.com/benchmarks/#instance-level-results) leaderboard.
Here we attempt to reproduce these results and investigate the effect of augmenting the original Citiscepas dataset
with synthetically generated data. We use **Chameleon AI Tools [Highwai simulator](https://www.mindtech.global/products)**
by [Mindtech](https://www.mindtech.global to produce these data.

We also used two approaches to improve the impact of using synthetic data:
1. Apply style-transfer using Cycle-GAN proposed in [*Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros*](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html)
2. Apply Domain-Adversarial Training approach (DANN) proposed in [*Domain-Adversarial Training of Neural Networks
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran√ßois Laviolette, Mario Marchand, Victor Lempitsky*](http://www.jmlr.org/papers/volume17/15-239/15-239.pdf).

The original [PANet repository](https://github.com/ShuLiu1993/PANet) is not well fit for training on Citiscapes,
so we produced [our fork of it](https://github.com/anakham/PANet/tree/Chameleon), that was used in experiments described below.

## PANet installation

See instructions in a [https://github.com/anakham/PANet/tree/Chameleon] repository.    

## Cityscapes data preparation and train

1. Download [Cityscapes dataset](https://www.cityscapes-dataset.com/downloads/): gtFine_trainvaltest.zip and
 leftImg8bit_trainvaltest.zip files.
2. Extract them into `../cityscapes/raw` directory, so that resulting directory structure should be:
    ```
    |--- PANet
    |    |--- ... panet files obtained by git clone ...
    |--- cityscapes
         |--- raw
              |--- gtFine
              |    |--- test
              |    |--- train
              |    |--- val
              |--- leftImg8bit     
                   |--- test
                   |--- train
                   |--- val
    ```
3. Start `PANet/lib/datasets/cityscapes/tools/convert_cityscapes_to_coco.py` script (from PANet directory):
    ```script
    python lib/datasets/cityscapes/tools/convert_cityscapes_to_coco.py --dataset cityscapes_instance_only --datadir /../cityscapes --outdir /../cityscapes
    ```
    It produces `annotations` and `images` subdirectories in a `cityscapes` directory.
4. Download Resnet50 pretrained model [see reference from the original PANet repository](https://drive.google.com/file/d/1-pVZQ3GR6Aj7KJzH9nWoRQ-Lts8IcdMS/view?usp=sharing)
   and place the `panet_mask_step179999.pth` file into `PANet/data/pretrained_model`  
5. Start PANet trainig:
    ```script
    python tools/train_net_step.py --dataset cityscapes --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml --use_tfboard --load_ckpt "data/pretrained_model/panet_mask_step179999.pth" --skip_top_layers
    ```
    Results will be stored in a `PANet/Output/<some_outdir>` folder.
6. Run validation script:
    ```script
    python tools/test_net.py --dataset cityscapes --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml --load_ckpt "Outputs/<data_dir>/ckpt/model_step47999.pth"    
    ```
    Resulting statistics will be calculated on 1) bound box predictions and 2) mask predictions 
    Results images will be saved in a `PANet/Output/<some_outdir>/test` folder.
7. Run test script:
    ```script
    rm -rf Output/<some_outdir>/test
    python tools/test_net.py --dataset cityscapes_test --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml --load_ckpt "Outputs/<data_dir>/ckpt/model_step47999.pth"    
    ```
    Results images will be saved `PANet/Output/<some_outdir>/test` folder, that can be archive and submitted to Cityscapes.
    Note, that you should remove `PANet/Output/<some_outdir>/test` folder before, because previous results for validation dataset 
    are stored there.
      
## Synthetic data preparation and usage

1. Synthetic data were prepared **Chameleon AI Tools
[Highwai simulator](https://www.mindtech.global/products)** by [Mindtech](https://www.mindtech.global).
This tool contains several predefined scenarios that can be played in an automatic mode. 
When a scenario is played, frames and annotations are stored in some folder. In this experiment, we 
used a predefined scenario named `gen_IntersectionWithPedestrians` and generated 1800 synthetic images (600 images
for 10-minute scenario with 1-second discretization, that was repeated 3 times at various light conditions).
We expect that a directory with results of *Highwai* simulation is placed alongside with cityscapes data 
in the following structure:
    ```
    |--- PANet
    |    |--- ...
    |--- cityscapes
    |    |--- ...
    |--- HighWai
         |--- raw
              |--- gen_IntersectionWithPedestrians
                   |--- dataset
                        |--- annotations.haf.csv
                        |--- boxes
                        |--- distance
                        |--- images
                        |--- masks
    ```
2. Run scenario for conversion of *Highwai* format to COCO
     ```script
    python lib/datasets/cityscapes/tools/convert_highwai_to_coco.py --datadir /../HighWai/raw --outdir /../HighWai
    ```
    It produces a `gen_IntersectionWithPedestrians.json` file with annotations in a `HighWai` directory.
3. Start PANet trainig:
    ```script
    python tools/train_net_step.py --dataset cityscapes_highwai --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml --use_tfboard --load_ckpt "data/pretrained_model/panet_mask_step179999.pth" --skip_top_layers
    ```
Later steps are similar to trainig on Cityscapes only, as described above. 

## Style transfer

We used an approach described in [*Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (by Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros)*](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html)
to convert a style of *HighWai* images to the style of Cityscapes dataset.
We use [**our fork**](https://github.com/IlyaOvodov/pytorch-CycleGAN-and-pix2pix) of this project that is slightly different from the [original one](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix): 
it allows scaling converted images to a required size (`--target_size` parameter) and don't add "_fake" to converted
images filenames, so that one can use generated images with an original *HighWai* annotation.

1. Install the *pytorch-CycleGAN-and-pix2pix* project:
    ```
    git clone https://github.com/IlyaOvodov/pytorch-CycleGAN-and-pix2pix
    cd pytorch-CycleGAN-and-pix2pix
    ```
2. Create a directory with data for training with 2 subfolders `trainA` and `trainB`. Make these folders either soft links to
*HighWai* generated images folder and Cityscapes images folder or copy images into these folders:
    ```
    CycleGAN_data
    |--- trainA
    |    |--- HighWai generated images *.png here
    |--- trainB
         |--- Cityscapes images *.png here 
     ```
3. Start *CycleGAN* trainig:
    ```
    python train.py --dataroot <CycleGAN_data folder> --name maps_cyclegan --model cycle_gan --max_dataset_size 5000 --display_id 0 --gpu_ids 0,1,2,3 --batch_size 12
    ```
    We trained GAN for 200 epochs.
4. Convert *HighWai* generated images using trained model:
    ```
    python test.py --dataroot ../HighWai/raw/gen_IntersectionWithPedestrians/dataset/images --name maps_cyclegan --model test --model_suffix _B --results_dir ./results  --eval --num_test 1000000 --no_dropout --preprocess none
    ```
    It will produce a `pytorch-CycleGAN-and-pix2pix/results2/maps_cyclegan/test_latest/images` folder with 
    original (*_real.png) and converted images.
5. Copy converted images to `HighWai/gan1` folder or create `HighWai/gan1` folder as a soft link to the directory created above.
6. Train PANet using both Cityscapes and HighWai converted images: 
    ```script
    python tools/train_net_step.py --dataset cityscapes_highwgan1 --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml --use_tfboard --load_ckpt "data/pretrained_model/panet_mask_step179999.pth" --skip_top_layers
    ```
    Later steps are similar to trainig on Cityscapes only.
    
We tried conversion *HighWai* images without *Identity loss* (see [original CycleGan paper](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html))
and with *Identity loss*.
Trainig with *Identity loss*, generating processed images can be invoked as follows:
```
python train.py --dataroot <CycleGAN_data folder> --name maps_cyclegan_l.1_lr.0005 --model cycle_gan --max_dataset_size 5000 --display_id 0 --gpu_ids 0,1,2,3 --batch_size 12 --lambda_identity 0.1 --lr 0.0005
python test.py --dataroot ../HighWai/raw/gen_IntersectionWithPedestrians/dataset/images --name maps_cyclegan_l.1_lr.0005 --model test --model_suffix _B --results_dir ./results  --eval --num_test 1000000 --no_dropout --preprocess none
```
The generated images should be placed at `HighWai/gan2`, then PANet training started:        
```script
python tools/train_net_step.py --dataset cityscapes_highwgan2 --cfg configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml --use_tfboard --load_ckpt "data/pretrained_model/panet_mask_step179999.pth" --skip_top_layers
```
    
## Domain-Adversarial Training
Domain-Adversarial Training ([DANN](http://www.jmlr.org/papers/volume17/15-239/15-239.pdf)) is implemented and can be
invoked by setting parameter `DANN.USE_DANN` to True either in a source code or in a parameters configuration file `configs/panet/e2e_panet_R-50-FPN_2x_mask_csc_from_coco.yaml`:
```buildoutcfg
DANN:
  USE_DANN: True
```   
Training and evaluation of PANet is similar to trainig with synthetic data descrived above.

## Results and conclusions
Our results are shown in the table below.

We couldn't rich the quality of the original PANet. We can expect it is caused by the fact that we used 4 GPUs for training
while original results were obtained on 8 GPUs. Although we took it into account and corrected the learning rate and number of iterations
properly, distributed batch normalization layers can not work in the same manner in this case.
Still, the effect of different approaches described above can be evaluated.

Main conclusions are:
* usage of even a small number of synthetic images (were used 1800 synthetic images along with 3000 real Cityscapes images)
gives significant boosting to recognition quality.
* style transfer with no Identity loss produces results in a worse quality then usage of unprocessed synthetic images
* using style transfer with Identity loss produces even more worse results. From our qualitative view, transformed images, in this case, are
much more similar to original Highwai generated images in color, etc. but loses some quality.
* Using DANN seems to make any improvement on local validation, but boosts results on test dataset, so it really helps to manage overfitting.  


| Experiment                             | val bbox AP | val bbox AP_50% | val mask AP | val mask AP_50% | test mask AP | test mask AP 50% |
|----------------------------------------|------------:|----------------:|------------:|----------------:|-------------:|-----------------:|
| *original, Citiscapes only train data* |             |                 |    0.365    |   0.629     | 31.7749 | 57.1443 |
| *original, COCO pretrained*            |             |                 |    0.414    |             | 36.4357 | 63.1375 |
| *reimplementation, Citiscapes only*    |    0.366    |    0.597	     |    0.317    |   0.590     | 27.8338 | 53.0382 |
| *reimplementation, COCO pretrained*    |    0.416    |    0.646        |    0.358    |   0.618     | 32.5255 | 57.8694 |
| *Citiscapes+Highwai generated*         |    0.428    |    0.670        |  **0.372**  | **0.644**   | 33.5782 | 59.5149 |
| *Citiscapes+Highwai with CycleGAN (no Identity loss)*  |**0.431**|**0.673**|    0.369    |   0.640     | 33.1721 | 59.2919 |
| *Citiscapes+Highwai with CycleGAN (with Identity loss)*| 0.422 | 0.656 |    0.365    |   0.634     | 33.2369 | 59.2844 |
| *Citiscapes+Highwai generated+DANN*    |    0.423    |    0.667        |    0.372    |   0.640     |**34.0316**|**60.5851**|
